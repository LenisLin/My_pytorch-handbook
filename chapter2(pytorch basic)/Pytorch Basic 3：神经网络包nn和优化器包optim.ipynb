{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch Basic 3：神经网络包nn和优化器包optim.ipynb","provenance":[],"authorship_tag":"ABX9TyPKj/EoHyjzpqdXrqVh9X3X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wkrcSjQhrVnR"},"source":["# PyTorch 基础 : 神经网络包nn和优化器optim\n","torch.nn是专门为神经网络设计的模块化接口。nn构建于Autograd之上，可用来定义和运行神经网络。\n","这里我们主要介绍几个一些常用的类。\n","\n","**约定：torch.nn 我们为了方便使用，会为他设置别名为nn，本章除nn以外还有其他的命名约定**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1O6lJ3Axq25i","executionInfo":{"status":"ok","timestamp":1637115410486,"user_tz":-480,"elapsed":27758,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"19d22211-c781-4536-bc26-fec622708749"},"source":["import torch\n","import torch.nn as nn\n","print(torch.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n"]}]},{"cell_type":"markdown","metadata":{"id":"8TRedlzesGQi"},"source":["除了nn别名以外，我们还引用了`nn.functional`，这个包中包含了神经网络中使用的一些常用函数，这些函数的特点是，`没有可学习的参数(如ReLU，pool，DropOut等)`，这些函数可以放在构造函数中，也可以不放，但是这里建议不放。\n","\n","一般情况下我们会**将nn.functional 设置为大写的F**，这样缩写方便调用"]},{"cell_type":"markdown","metadata":{"id":"_LHhI39NtB6N"},"source":["如果模型有`可学习的参数`时，最好使用`nn.Module`；\n","\n","否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用方式取决于个人喜好。\n","\n","由于`激活函数（ReLu、sigmoid、Tanh）`、`池化（MaxPool）`等层`没有可学习的参数`，可以使用对应的`functional`函数，而`卷积`、`全连接`等`有可学习参数`的网络建议使用`nn.Module`。\n","\n","虽然`dropout没有可学习参数`，但`建议还是使用nn.Dropout`而不是nn.functional.dropout，因为`dropout在训练和测试两个阶段的行为有所差别`，使用nn.Module对象能够通过model.eval操作加以区分。\n"]},{"cell_type":"code","metadata":{"id":"WgH7SVb1rP1b","executionInfo":{"status":"ok","timestamp":1637115974477,"user_tz":-480,"elapsed":518,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}}},"source":["import torch.nn.functional as F"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k-10R1t9t1qR"},"source":["## 定义一个网络\n","PyTorch中已经为我们准备好了现成的网络模型，只要`继承nn.Module`，并`实现它的forward方法`，PyTorch会根据autograd，`自动实现backward函数`，在`forward函数`中可使用`任何tensor支持的函数`，还可以使用`if、for循环、print、log等Python语法`，写法和标准的Python写法一致。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxf5_x3FuCGd","executionInfo":{"status":"ok","timestamp":1637116532891,"user_tz":-480,"elapsed":531,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"30fc7fa4-6c5d-4ce1-ad6d-99d772b00c59"},"source":["class Net(nn.Module): ## 继承nn.Module类，这样就可以使用其中设置好的网络了\n","  def __init__(self):\n","  ## nn.Module子类的函数必须在构造函数中执行父类的构造函数\n","    super(Net,self).__init__()\n","  ## 这里这个supre是因为Net作为nn.Module的子类，需要继承父类的初始化方法\n","    # 卷积层 '1'表示输入图片为单通道， '6'表示输出通道数(6个卷积核)，'3'表示卷积核为3*3\n","    self.conv1 = nn.Conv2d(1, 6, 3) \n","    #线性层，输入1350个特征，输出10个特征\n","    self.fc1  = nn.Linear(1350, 10)  #这里的1350是如何计算的呢？这就要看后面的forward函数\n","  \n","  ## forward propogation\n","  def forward(self, x): \n","    print(x.size()) ## 打印input的size,结果：[1, 1, 32, 32]\n","    # 卷积 -> 激活 -> 池化 \n","    x=self.conv1(x) ## 使得x通过第一个定义好的卷积层1\n","    x=F.relu(x) ## 卷积完后一般过一个relu\n","    print(x.size()) ## 结果：[1, 6, 30, 30]\n","\n","    x=F.max_pool2d(x,(2,2)) ## 池化，计算结果是15\n","    x = F.relu(x)\n","    print(x.size()) # 结果：[1, 6, 15, 15]\n","\n","    ## reshape，‘-1’表示自适应\n","    ## 这里做的就是压扁的操作 就是把后面的[1, 6, 15, 15]压扁，变为 [1, 1350]\n","    x = x.view(x.size()[0], -1) ## 变成一个列向量\n","    print(x.size()) # 这里就是fc1层的的输入1350 \n","    x = self.fc1(x)        \n","    return x\n","\n","net=Net()\n","print(net)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=1350, out_features=10, bias=True)\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"1dNkXhsiwFFk"},"source":["网络的可学习参数通过`net.parameters()`返回"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvjSx4_BwIFi","executionInfo":{"status":"ok","timestamp":1637116587252,"user_tz":-480,"elapsed":548,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"f68edd5b-3be8-47bc-9d30-7c26220659c3"},"source":["for parameters in net.parameters():\n","  print(parameters)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[[[-0.3132, -0.2879, -0.2701],\n","          [ 0.1968, -0.1522,  0.0222],\n","          [-0.3314, -0.2016,  0.2558]]],\n","\n","\n","        [[[ 0.0338, -0.2085, -0.1017],\n","          [-0.0640,  0.1522,  0.3060],\n","          [-0.2275, -0.1613, -0.1478]]],\n","\n","\n","        [[[-0.1396,  0.0140, -0.2937],\n","          [-0.1972,  0.1084,  0.3069],\n","          [ 0.1732, -0.1109,  0.2003]]],\n","\n","\n","        [[[-0.0607, -0.1636, -0.0384],\n","          [ 0.0381,  0.0806,  0.1708],\n","          [-0.1295,  0.0415, -0.2344]]],\n","\n","\n","        [[[ 0.2609, -0.2931,  0.1968],\n","          [ 0.0670,  0.3028,  0.2024],\n","          [ 0.2918,  0.2290, -0.2034]]],\n","\n","\n","        [[[-0.2479, -0.1746, -0.1352],\n","          [-0.3297,  0.2654, -0.0288],\n","          [-0.2869, -0.2976,  0.2436]]]], requires_grad=True)\n","Parameter containing:\n","tensor([ 0.2910, -0.0292, -0.2471, -0.1848,  0.1137,  0.0102],\n","       requires_grad=True)\n","Parameter containing:\n","tensor([[-0.0053,  0.0017,  0.0231,  ..., -0.0134, -0.0143,  0.0183],\n","        [ 0.0126, -0.0024,  0.0113,  ..., -0.0070,  0.0268, -0.0126],\n","        [-0.0058,  0.0086, -0.0056,  ..., -0.0203, -0.0220,  0.0112],\n","        ...,\n","        [-0.0022, -0.0146, -0.0028,  ..., -0.0252, -0.0200,  0.0141],\n","        [-0.0205,  0.0188,  0.0262,  ...,  0.0171,  0.0116,  0.0166],\n","        [-0.0270, -0.0063, -0.0192,  ...,  0.0222,  0.0262,  0.0247]],\n","       requires_grad=True)\n","Parameter containing:\n","tensor([-0.0074, -0.0099,  0.0211, -0.0193, -0.0258,  0.0213,  0.0166, -0.0138,\n","         0.0206,  0.0009], requires_grad=True)\n"]}]},{"cell_type":"markdown","metadata":{"id":"kIM0pfsBwH1S"},"source":["`net.named_parameters`可同时返回`可学习的参数`及`名称`。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MkdGH8ufxHk5","executionInfo":{"status":"ok","timestamp":1637116843538,"user_tz":-480,"elapsed":505,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"734536c1-dbf6-4a21-cb4e-5bb290c64143"},"source":["for name,parameters in net.named_parameters():\n","    print(name,':',parameters.size())"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["conv1.weight : torch.Size([6, 1, 3, 3])\n","conv1.bias : torch.Size([6])\n","fc1.weight : torch.Size([10, 1350])\n","fc1.bias : torch.Size([10])\n"]}]},{"cell_type":"markdown","metadata":{"id":"1kvbFnkTxHKC"},"source":["forward函数的输入和输出都是Tensor"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVZMbx3OxOPx","executionInfo":{"status":"ok","timestamp":1637116894918,"user_tz":-480,"elapsed":511,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"3515444c-2551-48b5-e38b-a8c7925d5362"},"source":["input = torch.randn(1, 1, 32, 32) # 这里的对应前面forward的输入是32\n","out = net(input)\n","out.size()\n"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 32, 32])\n","torch.Size([1, 6, 30, 30])\n","torch.Size([1, 6, 15, 15])\n","torch.Size([1, 1350])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 10])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPDJIMNmxZq5","executionInfo":{"status":"ok","timestamp":1637116915509,"user_tz":-480,"elapsed":689,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"d3e4decd-8482-4749-fa4c-844a4d796db5"},"source":["input.size()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 1, 32, 32])"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"tpLcL_Nyxdj5"},"source":["在反向传播前，先要将所有`参数的梯度清零`"]},{"cell_type":"code","metadata":{"id":"sm1C22qOxcIB","executionInfo":{"status":"ok","timestamp":1637116947700,"user_tz":-480,"elapsed":505,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}}},"source":["net.zero_grad() \n","out.backward(torch.ones(1,10)) # 反向传播的实现是PyTorch自动实现的，我们只要调用这个函数即可"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jzrF61dvxkzB"},"source":["**注意**:torch.nn只支持mini-batches，不支持一次只输入一个样本，即`一次必须是一个batch`。\n","\n","也就是说，就算我们输入一个样本，也会对样本进行分批，所以，所有的输入都会增加一个维度，我们对比下刚才的input，nn中定义为3维，但是我们人工创建时多`增加了一个维度`，变为了4维，最`前面的1即为batch-size`"]},{"cell_type":"markdown","metadata":{"id":"7w7tVsVtyRFx"},"source":["## 损失函数\n","在`nn`中PyTorch还预制了常用的`损失函数`，下面我们用`MSELoss`用来计算均方误差"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ztzdjoN8zTtS","executionInfo":{"status":"ok","timestamp":1637117467717,"user_tz":-480,"elapsed":524,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"8c1c613a-cb3e-4f4d-9a92-cc4c6f0db27f"},"source":["y = torch.arange(0,10).view(1,10).float() ## 虚拟定义一个y，作为监督学习样本的label\n","loss_fn = nn.MSELoss()\n","loss = loss_fn(out, y)\n","#loss是个scalar，我们可以直接用item获取到他的python类型的数值\n","print(loss.item()) "],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["25.067508697509766\n"]}]},{"cell_type":"markdown","metadata":{"id":"rxroJCu1zma5"},"source":["并且这里loss是一个标量，所以可以直接`loss.backward()`"]},{"cell_type":"markdown","metadata":{"id":"F7Uxw3btzsGb"},"source":["## 优化器\n","在反向传播计算完所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法(SGD)的更新策略如下：\n","\n","`weight = weight - learning_rate * gradient`\n","\n","在`torch.optim`中实现大多数的优化方法，例如`RMSProp`、`Adam`、`SGD`等，下面我们使用SGD做个简单的样例"]},{"cell_type":"code","metadata":{"id":"7WGuNzz5zznR","executionInfo":{"status":"ok","timestamp":1637117550997,"user_tz":-480,"elapsed":520,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}}},"source":["import torch.optim"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzi9fTepz2tJ","executionInfo":{"status":"ok","timestamp":1637117598271,"user_tz":-480,"elapsed":515,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"7af6f480-72da-4986-ac48-6a108215b31a"},"source":["out = net(input) # 这里调用的时候会打印出我们在forword函数中打印的x的大小\n","loss_fn = nn.MSELoss()\n","loss = loss_fn(out, y)\n","#新建一个优化器，SGD只需要要调整的参数和学习率\n","optimizer = torch.optim.SGD(net.parameters(), lr = 0.01,momentum=0.9) ## 带动量SGD\n","# 先梯度清零(与net.zero_grad()效果一样)\n","optimizer.zero_grad() \n","loss.backward()\n","\n","#更新参数\n","optimizer.step()"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 32, 32])\n","torch.Size([1, 6, 30, 30])\n","torch.Size([1, 6, 15, 15])\n","torch.Size([1, 1350])\n"]}]},{"cell_type":"markdown","metadata":{"id":"f7GmMoHxzyoy"},"source":["这样，神经网络的数据的一个完整的传播就已经通过PyTorch实现了，下面一章将介绍PyTorch提供的数据加载和处理工具，使用这些工具可以方便的处理所需要的数据。\n","\n","看完这节，大家可能对神经网络模型里面的一些参数的计算方式还有疑惑，这部分会在第二章 第四节 卷积神经网络有详细介绍，并且在第三章 第二节 MNIST数据集手写数字识别的实践代码中有详细的注释说明。"]}]}