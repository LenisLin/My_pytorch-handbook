{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch Basic 1：张量（Tensor）.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNe5Eson7Jt/KUkj6sdK0Yx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"o2k4iATx-FPr"},"source":["# Pytorch Basic 1：张量（Tensor）"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1kUokMd-C20","executionInfo":{"status":"ok","timestamp":1637053318911,"user_tz":-480,"elapsed":28086,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"75ee15be-f2c8-40d9-c99c-32e5d12e6d26"},"source":["## load packages\n","import torch\n","import numpy as np\n","\n","## 看一下torch 版本\n","print(torch.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n"]}]},{"cell_type":"markdown","metadata":{"id":"4ugh0z3G-2WO"},"source":["## 张量(Tensor)\n","张量的英文是Tensor，它是PyTorch里面基础的运算单位，与Numpy的ndarray相同都表示的是一个多维的矩阵。\n","\n","与ndarray的最大区别就是，PyTorch的Tensor可以在``GPU``上运行，而 numpy 的 ndarray 只能在 CPU 上运行，在GPU上运行大大加快了运算速度。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qp2E-g4b_Bym"},"source":["### 创建Tensor"]},{"cell_type":"code","metadata":{"id":"o9mncggm-vcx","executionInfo":{"status":"ok","timestamp":1637053467702,"user_tz":-480,"elapsed":4,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}}},"source":["x=torch.rand(2,3) ## 生成一个2*3的tensor\n","y=torch.rand(2,3,4,5) ## 生成一个2*3*4*5的多维tensor"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lGUKCfghCwvH"},"source":["#### 其他初始化方法"]},{"cell_type":"code","metadata":{"id":"XApiEQweAVVu"},"source":["zero=torch.zeros(2,2) ## 2*2 tensor，元素全为0\n","ones=torch.ones(2,2) ## 2*2 tensor,元素全为1\n","eye=torch.eye(2,2) ## 2*2 tensor,单位矩阵\n","rand=torch.rand(2,2) ## 2*2 tensor，元素大小随机"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oqJ-K_9jASrB"},"source":["###查看Tensor的维度"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bGDqOVsAcJW","executionInfo":{"status":"ok","timestamp":1637053790336,"user_tz":-480,"elapsed":515,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"81bb7ead-b9f7-425b-8f96-f42dedc00a67"},"source":["print(x.size()) ## 通过 .size()方法查看维度\n","print(np.shape(y)) ## 通过np.shape()函数来查看"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3])\n","torch.Size([2, 3, 4, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"vtPSVfBg_Y-X"},"source":["在同构的意义下:\n","\n","第零阶张量 （r = 0） 为标量 （Scalar）\n","\n","第一阶张量 （r = 1） 为向量 （Vector）\n","\n","第二阶张量 （r = 2） 则成为矩阵 （Matrix）\n","\n","第三阶以上的统称为多维张量。\n","\n","其中要特别注意的就是标量，我们先生成一个标量：\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_8QSjmyA_hof","executionInfo":{"status":"ok","timestamp":1637053671174,"user_tz":-480,"elapsed":517,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"2c60edcd-5cda-487b-f76a-88f836bc431c"},"source":["scalar=torch.tensor(1.433223) ## 直接用一个已有的数据生成tensor\n","print(scalar)\n","\n","\n","print(scalar.item()) ## 用 .item()方法查看一个标量对应的python对象数值\n","print(type(scalar.item()),'\\n') ## <class 'float'> \n","\n","tensor = torch.tensor([3.1433223]) ## 特别的,如果张量中只有一个元素的tensor也可以调用`tensor.item`方法\n","print(tensor)\n","print(tensor.item())"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.4332)\n","1.433223009109497\n","<class 'float'> \n","\n","tensor([3.1433])\n","3.143322229385376\n"]}]},{"cell_type":"markdown","metadata":{"id":"mDx4sMmvAqMd"},"source":["###基本类型\n","Tensor的基本数据类型有五种：\n","- 32位浮点型：torch.FloatTensor。 (默认)\n","- 64位整型：torch.LongTensor。\n","- 32位整型：torch.IntTensor。\n","- 16位整型：torch.ShortTensor。\n","- 64位浮点型：torch.DoubleTensor。\n","\n","除以上数字类型外，还有byte和chart型"]},{"cell_type":"markdown","metadata":{"id":"56b5zdq-A3ft"},"source":["## Tensor和Numpy的转换\n","使用numpy方法将Tensor转为ndarray"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sbx92RDXBAdt","executionInfo":{"status":"ok","timestamp":1637053965942,"user_tz":-480,"elapsed":520,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"6361266f-a4be-469c-b4f3-6be0dbbc82c1"},"source":["a_tensor=torch.randn(3,2)\n","print(a_tensor)\n","a_numpy=a_tensor.numpy() ## 将tensor转化为numpy\n","print(a_numpy)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.0920,  1.1896],\n","        [-1.5687, -1.0908],\n","        [-0.0830, -0.5482]])\n","[[ 0.09199096  1.1895736 ]\n"," [-1.5686744  -1.0907832 ]\n"," [-0.08302718 -0.5481665 ]]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v3ybyTvpBT8P","executionInfo":{"status":"ok","timestamp":1637054023457,"user_tz":-480,"elapsed":540,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"7b1783e8-6e58-4392-c139-0bf3763963e1"},"source":["tensor_a=torch.tensor(a_numpy) ## 将numpy转化成tensor\n","tensor_a=torch.from_numpy(a_numpy) ## 效果一样\n","print(tensor_a) "],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.0920,  1.1896],\n","        [-1.5687, -1.0908],\n","        [-0.0830, -0.5482]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"kkAYuQGABw-X"},"source":["Tensor和numpy对象***共享内存***，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果***其中一个变了，另外一个也会随之改变***。（具体查看60 minutes）"]},{"cell_type":"markdown","metadata":{"id":"WMJN3anXB7ie"},"source":["## 设备间转换\n","一般情况下可以使用.cuda方法将tensor移动到gpu，这步操作需要cuda设备支持"]},{"cell_type":"code","metadata":{"id":"-hZorxqjB6fQ"},"source":["print(torch.cuda.is_available()) ## 查看cuda是否可用\n","\n","cpu_a=torch.rand(4, 3)\n","cpu_a.type()\n","\n","gpu_a=cpu_a.cuda() ## 用 .cuda()方法把tensor移到GPU中\n","gpu_a.type()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","gpu_a=cpu_a.to(device) ## 和 .cuda()方法一样，不过更推荐这种用法，可以在多GPU情况下指定GPU\n","gpu_a.type()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j08p8OiBDUYO"},"source":["## 常用方法\n","PyTorch中对张量的操作api 和 NumPy 非常相似，如果熟悉 NumPy 中的操作，那么他们二者基本是一致的："]},{"cell_type":"code","metadata":{"id":"9XQpHCXJDT6Y"},"source":["x=torch.rand(5,3)\n","y=torch.rand(5,3)\n","\n","print(x,'\\n',y)\n","\n","## 和，差\n","print(x+y,end='\\n') ## 同torch.add(x,y,out=result)，即把x+y结果赋值给result\n","print(x-y,end='\\n')\n","\n","## 加+替换\n","print(y,'\\n')\n","y.add_(x)\n","print(y,'\\n') ## 这里的y是上面的y+x的结果\n","\n","## 内积(点乘)\n","u=torch.rand(3,2) \n","v=torch.rand(3,2)\n","print(u.mul(v)) ## 返回仍然是3*2矩阵\n","\n","## 外积（叉乘）\n","v1=torch.rand(2,3)\n","print(u.mm(v1)) ## 返回3*3矩阵\n","\n","## view改变tensor维度\n","x=torch.rand(5,3)\n","print(x)\n","y=x.view(15)\n","print(np.shape(y))\n","z=x.view(3,5)\n","print(z) ## 这里和转置不一样，是近似于先把原来的tensor转化成一个list，然后再重新按照行列要求生成一个tensor\n","\n","print(x.view(-1,15)) ## 这里的-1是我们不想算有几行，让电脑帮我们算\n","\n","## 沿着行取最大值\n","max_value, max_idx = torch.max(x, dim=1)\n","print(max_value, max_idx)\n","\n","## 对行求和\n","sum_x = torch.sum(x, dim=1)\n","print(sum_x)"],"execution_count":null,"outputs":[]}]}