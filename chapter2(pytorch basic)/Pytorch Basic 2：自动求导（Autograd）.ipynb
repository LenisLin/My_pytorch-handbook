{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch Basic 2：自动求导（Autograd）.ipynb","provenance":[],"authorship_tag":"ABX9TyMBmJYnUFjFsjv0AcRGUgwy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"un9RLGBsWh8a"},"source":["# Pytorch Basic 2：自动求梯度（Autograd）\n","\n","PyTorch的``Autograd``模块实现了深度学习的算法中的反向传播求导数，在``张量（Tensor类）``上的所有操作，Autograd都能为他们自动提供微分，简化了手动计算导数的复杂过程。\n","\n","在0.4以前的版本中，Pytorch使用Variable类来自动计算所有的梯度Variable类主要包含三个属性：\n","\n","data：保存Variable所包含的Tensor；\n","\n","``grad``：保存``data对应的梯度``，grad也是个Variable，而不是Tensor，它和data的形状一样；\n","\n","``grad_fn``：指向一个Function对象，这个Function用来反向传播计算输入的梯度。\n","\n","从0.4起， Variable 正式合并入Tensor类，通过Variable嵌套实现的自动微分功能已经整合进入了Tensor类中。\n","\n","虽然为了代码的兼容性还是可以使用Variable(tensor)这种方式进行嵌套，但是这个操作其实什么都没做。\n","\n","所以，以后的代码建议直接使用Tensor类进行操作，因为官方文档中已经将Variable设置成过期模块。\n","\n","要想通过``Tensor``类本身就支持了使用``autograd``功能，只需要设置.``requires_grad=True``\n","\n","Variable类中的的`grad`和`grad_fn`属性已经整合进入了Tensor类中\n","\n","即，现在直接用Tensor就可以自动求梯度了"]},{"cell_type":"code","metadata":{"id":"6pnNPvMmWesF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637113821239,"user_tz":-480,"elapsed":4306,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"f8eda48a-16b8-4e28-856a-342c4d61a01f"},"source":["import torch\n","print(torch.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n"]}]},{"cell_type":"markdown","metadata":{"id":"6w_vsscsjfDe"},"source":["## Autograd\n","在张量创建时，通过设置``requires_grad=Ture``来告诉Pytorch需要对该张量进行自动求导，PyTorch会记录该张量的每一步操作历史并自动计算"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvFFbOI2kg7G","executionInfo":{"status":"ok","timestamp":1637113821240,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"da200eb3-9631-428f-a6f5-8502158267ef"},"source":["x=torch.rand(5,5,requires_grad=True)\n","print(x)\n","\n","y=torch.rand(5,5,requires_grad=True)\n","print(y)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.8984, 0.1947, 0.9008, 0.3528, 0.4726],\n","        [0.0042, 0.3750, 0.1714, 0.6831, 0.3804],\n","        [0.3625, 0.2809, 0.1034, 0.8454, 0.8264],\n","        [0.5135, 0.1154, 0.9782, 0.2369, 0.4671],\n","        [0.4736, 0.2118, 0.4727, 0.4840, 0.4009]], requires_grad=True)\n","tensor([[0.0084, 0.7092, 0.7782, 0.3673, 0.2350],\n","        [0.9813, 0.9417, 0.0787, 0.4513, 0.1523],\n","        [0.9955, 0.8125, 0.6706, 0.0885, 0.1878],\n","        [0.5417, 0.2326, 0.1560, 0.7495, 0.6755],\n","        [0.7615, 0.0289, 0.7057, 0.2156, 0.7761]], requires_grad=True)\n"]}]},{"cell_type":"markdown","metadata":{"id":"NOyW90agkuFd"},"source":["PyTorch会自动追踪和记录对与张量的所有操作，当计算完成后调用`.backward()`方法自动计算梯度并且将计算结果保存到`grad属性`中。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pxka8LUgk7p1","executionInfo":{"status":"ok","timestamp":1637113825306,"user_tz":-480,"elapsed":550,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"5a05fb5d-c4e9-44e1-8dd2-b6d98e31d71d"},"source":["z=torch.sum(x+y) ## torch.sum() 对一个tensor中所有元素求和\n","print(z) ## 注意此时的z是一个标量"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(23.5075, grad_fn=<SumBackward0>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"SRXBsDy6lYN2"},"source":["在张量进行操作后，`grad_fn`已经被赋予了一个新的函数，这个函数引用了一个创建了这个`Tensor类`的`Function对象`。\n","Tensor和Function互相连接生成了一个非循环图(计算节点)，它记录并且编码了完整的计算历史。每个`张量`都有一个`.grad_fn属性`，如果这个`张量是用户手动创建`的那么这个张量的`grad_fn是None`。\n","\n","下面我们来调用反向传播函数，计算其梯度"]},{"cell_type":"markdown","metadata":{"id":"j3iP_Ad2ly6O"},"source":["## 简单的自动求导"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U1C4bm3hl0X3","executionInfo":{"status":"ok","timestamp":1637113826654,"user_tz":-480,"elapsed":8,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"3c85b2aa-bdf0-4bb1-c36b-59133a58d057"},"source":["z.backward()\n","print(x.grad,'\\t',y.grad) ## 由于z是一个标量，且x,y是叶子节点，所以可以完成自动求导"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.]]) \t tensor([[1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"dTUlnHIumIK3"},"source":["如果`Tensor`类表示的是一个`标量`（即它`包含一个元素的张量`），则`不需要`为backward()指定任何参数，但是如果它`有更多的元素`，则`需要指定一个gradient参数`，它是`形状匹配的张量`。\n","\n","以上的 `z.backward()`相当于是`z.backward(torch.tensor(1.))`的简写。\n","这种参数常出现在图像分类中的单标签分类，输出一个标量代表图像的标签。\n","\n","ps：在实际的网络中，我们一般是定义一个loss，然后正向传播得到这个loss，loss就是一个标量，那时候直接`loss.backward()`就可以了"]},{"cell_type":"markdown","metadata":{"id":"WiYRNb0Mmq-d"},"source":["## 复杂的自动求导"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TCI43ZEDms8V","executionInfo":{"status":"ok","timestamp":1637113828473,"user_tz":-480,"elapsed":4,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"1f6f9738-491f-428f-ca2c-c6b1c75f7e61"},"source":["x = torch.rand(5, 5, requires_grad=True)\n","y = torch.rand(5, 5, requires_grad=True)\n","z= x**2+y**3\n","print(z.size())"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"ASTMld9Um0GW"},"source":["我们的返回值(z)不是一个标量，所以需要输入一个`大小相同`的张量作为参数，这里我们用`ones_like`函数根据x生成一个张量"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_25Vceym6Rm","executionInfo":{"status":"ok","timestamp":1637113831922,"user_tz":-480,"elapsed":647,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"3ae5deec-37c5-40bc-9810-f1b598ee45e7"},"source":["z.backward(torch.ones_like(x)) ## 和x同型的全为1的tensor\n","print(x.grad) ## 求Jacobian矩阵\n","#print(y.grad) ## 会报错"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.6700, 1.6516, 1.4439, 0.9153, 0.2578],\n","        [0.5267, 1.3706, 1.4177, 0.2592, 0.2190],\n","        [0.2655, 0.9147, 1.3577, 0.3641, 1.7151],\n","        [1.1804, 1.3216, 0.5222, 0.5130, 0.1340],\n","        [0.3530, 1.2496, 0.3577, 1.8426, 0.0355]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"K_nZAdc8nZA7"},"source":["我们可以使用`with torch.no_grad()`上下文管理器`临时禁止`对已设置`requires_grad=True`的张量进行`自动求导`。这个方法在`测试集计算准确率`的时候会经常用到。\n","\n","例如："]},{"cell_type":"code","metadata":{"id":"CS_M-rryni-x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637113835838,"user_tz":-480,"elapsed":539,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"adcce273-9877-43d3-b547-06c24f2fd460"},"source":["with torch.no_grad():\n","    print((x+y*2).requires_grad)\n","\n","## 在测试集计算准确率的时候，一样进行正向传播，如果没有with torch.no_grad()，pytorch还是会去计算那些被设置为需要计算梯度的tensor"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","metadata":{"id":"q7aA-zJ3n3P8"},"source":["使用`.no_grad()`进行嵌套后，代码不会跟踪历史记录，也就是说保存的这部分记录会减少内存的使用量并且会加快少许的运算速度。"]},{"cell_type":"markdown","metadata":{"id":"P1ffzIeZn4wk"},"source":["## Autograd 过程解析\n","为了说明Pytorch的自动求导原理，我们来尝试分析一下PyTorch的源代码，虽然Pytorch的 Tensor和 TensorBase都是使用CPP来实现的，但是可以使用一些Python的一些方法查看这些对象在Python的属性和状态。\n"," Python的 `dir()` 返回参数的属性、方法列表。`z`是一个Tensor变量，看看里面有哪些成员变量。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZW6PtiYGn-Rs","executionInfo":{"status":"ok","timestamp":1637064119190,"user_tz":-480,"elapsed":809,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"d50b2975-c9f7-429a-acfc-90a97b982401"},"source":["dir(z)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['T',\n"," '__abs__',\n"," '__add__',\n"," '__and__',\n"," '__array__',\n"," '__array_priority__',\n"," '__array_wrap__',\n"," '__bool__',\n"," '__class__',\n"," '__complex__',\n"," '__contains__',\n"," '__deepcopy__',\n"," '__delattr__',\n"," '__delitem__',\n"," '__dict__',\n"," '__dir__',\n"," '__div__',\n"," '__dlpack__',\n"," '__dlpack_device__',\n"," '__doc__',\n"," '__eq__',\n"," '__float__',\n"," '__floordiv__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__getitem__',\n"," '__gt__',\n"," '__hash__',\n"," '__iadd__',\n"," '__iand__',\n"," '__idiv__',\n"," '__ifloordiv__',\n"," '__ilshift__',\n"," '__imod__',\n"," '__imul__',\n"," '__index__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__int__',\n"," '__invert__',\n"," '__ior__',\n"," '__ipow__',\n"," '__irshift__',\n"," '__isub__',\n"," '__iter__',\n"," '__itruediv__',\n"," '__ixor__',\n"," '__le__',\n"," '__len__',\n"," '__long__',\n"," '__lshift__',\n"," '__lt__',\n"," '__matmul__',\n"," '__mod__',\n"," '__module__',\n"," '__mul__',\n"," '__ne__',\n"," '__neg__',\n"," '__new__',\n"," '__nonzero__',\n"," '__or__',\n"," '__pos__',\n"," '__pow__',\n"," '__radd__',\n"," '__rand__',\n"," '__rdiv__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__reversed__',\n"," '__rfloordiv__',\n"," '__rlshift__',\n"," '__rmatmul__',\n"," '__rmod__',\n"," '__rmul__',\n"," '__ror__',\n"," '__rpow__',\n"," '__rrshift__',\n"," '__rshift__',\n"," '__rsub__',\n"," '__rtruediv__',\n"," '__rxor__',\n"," '__setattr__',\n"," '__setitem__',\n"," '__setstate__',\n"," '__sizeof__',\n"," '__str__',\n"," '__sub__',\n"," '__subclasshook__',\n"," '__torch_function__',\n"," '__truediv__',\n"," '__weakref__',\n"," '__xor__',\n"," '_backward_hooks',\n"," '_base',\n"," '_cdata',\n"," '_coalesced_',\n"," '_conj',\n"," '_conj_physical',\n"," '_dimI',\n"," '_dimV',\n"," '_fix_weakref',\n"," '_grad',\n"," '_grad_fn',\n"," '_indices',\n"," '_is_view',\n"," '_make_subclass',\n"," '_neg_view',\n"," '_nnz',\n"," '_python_dispatch',\n"," '_reduce_ex_internal',\n"," '_update_names',\n"," '_values',\n"," '_version',\n"," 'abs',\n"," 'abs_',\n"," 'absolute',\n"," 'absolute_',\n"," 'acos',\n"," 'acos_',\n"," 'acosh',\n"," 'acosh_',\n"," 'add',\n"," 'add_',\n"," 'addbmm',\n"," 'addbmm_',\n"," 'addcdiv',\n"," 'addcdiv_',\n"," 'addcmul',\n"," 'addcmul_',\n"," 'addmm',\n"," 'addmm_',\n"," 'addmv',\n"," 'addmv_',\n"," 'addr',\n"," 'addr_',\n"," 'align_as',\n"," 'align_to',\n"," 'all',\n"," 'allclose',\n"," 'amax',\n"," 'amin',\n"," 'aminmax',\n"," 'angle',\n"," 'any',\n"," 'apply_',\n"," 'arccos',\n"," 'arccos_',\n"," 'arccosh',\n"," 'arccosh_',\n"," 'arcsin',\n"," 'arcsin_',\n"," 'arcsinh',\n"," 'arcsinh_',\n"," 'arctan',\n"," 'arctan_',\n"," 'arctanh',\n"," 'arctanh_',\n"," 'argmax',\n"," 'argmin',\n"," 'argsort',\n"," 'as_strided',\n"," 'as_strided_',\n"," 'as_subclass',\n"," 'asin',\n"," 'asin_',\n"," 'asinh',\n"," 'asinh_',\n"," 'atan',\n"," 'atan2',\n"," 'atan2_',\n"," 'atan_',\n"," 'atanh',\n"," 'atanh_',\n"," 'backward',\n"," 'baddbmm',\n"," 'baddbmm_',\n"," 'bernoulli',\n"," 'bernoulli_',\n"," 'bfloat16',\n"," 'bincount',\n"," 'bitwise_and',\n"," 'bitwise_and_',\n"," 'bitwise_left_shift',\n"," 'bitwise_left_shift_',\n"," 'bitwise_not',\n"," 'bitwise_not_',\n"," 'bitwise_or',\n"," 'bitwise_or_',\n"," 'bitwise_right_shift',\n"," 'bitwise_right_shift_',\n"," 'bitwise_xor',\n"," 'bitwise_xor_',\n"," 'bmm',\n"," 'bool',\n"," 'broadcast_to',\n"," 'byte',\n"," 'cauchy_',\n"," 'cdouble',\n"," 'ceil',\n"," 'ceil_',\n"," 'cfloat',\n"," 'char',\n"," 'cholesky',\n"," 'cholesky_inverse',\n"," 'cholesky_solve',\n"," 'chunk',\n"," 'clamp',\n"," 'clamp_',\n"," 'clamp_max',\n"," 'clamp_max_',\n"," 'clamp_min',\n"," 'clamp_min_',\n"," 'clip',\n"," 'clip_',\n"," 'clone',\n"," 'coalesce',\n"," 'col_indices',\n"," 'conj',\n"," 'conj_physical',\n"," 'conj_physical_',\n"," 'contiguous',\n"," 'copy_',\n"," 'copysign',\n"," 'copysign_',\n"," 'corrcoef',\n"," 'cos',\n"," 'cos_',\n"," 'cosh',\n"," 'cosh_',\n"," 'count_nonzero',\n"," 'cov',\n"," 'cpu',\n"," 'cross',\n"," 'crow_indices',\n"," 'cuda',\n"," 'cummax',\n"," 'cummin',\n"," 'cumprod',\n"," 'cumprod_',\n"," 'cumsum',\n"," 'cumsum_',\n"," 'data',\n"," 'data_ptr',\n"," 'deg2rad',\n"," 'deg2rad_',\n"," 'dense_dim',\n"," 'dequantize',\n"," 'det',\n"," 'detach',\n"," 'detach_',\n"," 'device',\n"," 'diag',\n"," 'diag_embed',\n"," 'diagflat',\n"," 'diagonal',\n"," 'diff',\n"," 'digamma',\n"," 'digamma_',\n"," 'dim',\n"," 'dist',\n"," 'div',\n"," 'div_',\n"," 'divide',\n"," 'divide_',\n"," 'dot',\n"," 'double',\n"," 'dsplit',\n"," 'dtype',\n"," 'eig',\n"," 'element_size',\n"," 'eq',\n"," 'eq_',\n"," 'equal',\n"," 'erf',\n"," 'erf_',\n"," 'erfc',\n"," 'erfc_',\n"," 'erfinv',\n"," 'erfinv_',\n"," 'exp',\n"," 'exp2',\n"," 'exp2_',\n"," 'exp_',\n"," 'expand',\n"," 'expand_as',\n"," 'expm1',\n"," 'expm1_',\n"," 'exponential_',\n"," 'fill_',\n"," 'fill_diagonal_',\n"," 'fix',\n"," 'fix_',\n"," 'flatten',\n"," 'flip',\n"," 'fliplr',\n"," 'flipud',\n"," 'float',\n"," 'float_power',\n"," 'float_power_',\n"," 'floor',\n"," 'floor_',\n"," 'floor_divide',\n"," 'floor_divide_',\n"," 'fmax',\n"," 'fmin',\n"," 'fmod',\n"," 'fmod_',\n"," 'frac',\n"," 'frac_',\n"," 'frexp',\n"," 'gather',\n"," 'gcd',\n"," 'gcd_',\n"," 'ge',\n"," 'ge_',\n"," 'geometric_',\n"," 'geqrf',\n"," 'ger',\n"," 'get_device',\n"," 'grad',\n"," 'grad_fn',\n"," 'greater',\n"," 'greater_',\n"," 'greater_equal',\n"," 'greater_equal_',\n"," 'gt',\n"," 'gt_',\n"," 'half',\n"," 'hardshrink',\n"," 'has_names',\n"," 'heaviside',\n"," 'heaviside_',\n"," 'histc',\n"," 'histogram',\n"," 'hsplit',\n"," 'hypot',\n"," 'hypot_',\n"," 'i0',\n"," 'i0_',\n"," 'igamma',\n"," 'igamma_',\n"," 'igammac',\n"," 'igammac_',\n"," 'imag',\n"," 'index_add',\n"," 'index_add_',\n"," 'index_copy',\n"," 'index_copy_',\n"," 'index_fill',\n"," 'index_fill_',\n"," 'index_put',\n"," 'index_put_',\n"," 'index_select',\n"," 'indices',\n"," 'inner',\n"," 'int',\n"," 'int_repr',\n"," 'inverse',\n"," 'is_coalesced',\n"," 'is_complex',\n"," 'is_conj',\n"," 'is_contiguous',\n"," 'is_cuda',\n"," 'is_distributed',\n"," 'is_floating_point',\n"," 'is_inference',\n"," 'is_leaf',\n"," 'is_meta',\n"," 'is_mkldnn',\n"," 'is_mlc',\n"," 'is_neg',\n"," 'is_nonzero',\n"," 'is_ort',\n"," 'is_pinned',\n"," 'is_quantized',\n"," 'is_same_size',\n"," 'is_set_to',\n"," 'is_shared',\n"," 'is_signed',\n"," 'is_sparse',\n"," 'is_sparse_csr',\n"," 'is_vulkan',\n"," 'is_xpu',\n"," 'isclose',\n"," 'isfinite',\n"," 'isinf',\n"," 'isnan',\n"," 'isneginf',\n"," 'isposinf',\n"," 'isreal',\n"," 'istft',\n"," 'item',\n"," 'kron',\n"," 'kthvalue',\n"," 'layout',\n"," 'lcm',\n"," 'lcm_',\n"," 'ldexp',\n"," 'ldexp_',\n"," 'le',\n"," 'le_',\n"," 'lerp',\n"," 'lerp_',\n"," 'less',\n"," 'less_',\n"," 'less_equal',\n"," 'less_equal_',\n"," 'lgamma',\n"," 'lgamma_',\n"," 'log',\n"," 'log10',\n"," 'log10_',\n"," 'log1p',\n"," 'log1p_',\n"," 'log2',\n"," 'log2_',\n"," 'log_',\n"," 'log_normal_',\n"," 'log_softmax',\n"," 'logaddexp',\n"," 'logaddexp2',\n"," 'logcumsumexp',\n"," 'logdet',\n"," 'logical_and',\n"," 'logical_and_',\n"," 'logical_not',\n"," 'logical_not_',\n"," 'logical_or',\n"," 'logical_or_',\n"," 'logical_xor',\n"," 'logical_xor_',\n"," 'logit',\n"," 'logit_',\n"," 'logsumexp',\n"," 'long',\n"," 'lstsq',\n"," 'lt',\n"," 'lt_',\n"," 'lu',\n"," 'lu_solve',\n"," 'map2_',\n"," 'map_',\n"," 'masked_fill',\n"," 'masked_fill_',\n"," 'masked_scatter',\n"," 'masked_scatter_',\n"," 'masked_select',\n"," 'matmul',\n"," 'matrix_exp',\n"," 'matrix_power',\n"," 'max',\n"," 'maximum',\n"," 'mean',\n"," 'median',\n"," 'min',\n"," 'minimum',\n"," 'mm',\n"," 'mode',\n"," 'moveaxis',\n"," 'movedim',\n"," 'msort',\n"," 'mul',\n"," 'mul_',\n"," 'multinomial',\n"," 'multiply',\n"," 'multiply_',\n"," 'mv',\n"," 'mvlgamma',\n"," 'mvlgamma_',\n"," 'name',\n"," 'names',\n"," 'nan_to_num',\n"," 'nan_to_num_',\n"," 'nanmean',\n"," 'nanmedian',\n"," 'nanquantile',\n"," 'nansum',\n"," 'narrow',\n"," 'narrow_copy',\n"," 'ndim',\n"," 'ndimension',\n"," 'ne',\n"," 'ne_',\n"," 'neg',\n"," 'neg_',\n"," 'negative',\n"," 'negative_',\n"," 'nelement',\n"," 'new',\n"," 'new_empty',\n"," 'new_empty_strided',\n"," 'new_full',\n"," 'new_ones',\n"," 'new_tensor',\n"," 'new_zeros',\n"," 'nextafter',\n"," 'nextafter_',\n"," 'nonzero',\n"," 'norm',\n"," 'normal_',\n"," 'not_equal',\n"," 'not_equal_',\n"," 'numel',\n"," 'numpy',\n"," 'orgqr',\n"," 'ormqr',\n"," 'outer',\n"," 'output_nr',\n"," 'permute',\n"," 'pin_memory',\n"," 'pinverse',\n"," 'polygamma',\n"," 'polygamma_',\n"," 'positive',\n"," 'pow',\n"," 'pow_',\n"," 'prelu',\n"," 'prod',\n"," 'put',\n"," 'put_',\n"," 'q_per_channel_axis',\n"," 'q_per_channel_scales',\n"," 'q_per_channel_zero_points',\n"," 'q_scale',\n"," 'q_zero_point',\n"," 'qr',\n"," 'qscheme',\n"," 'quantile',\n"," 'rad2deg',\n"," 'rad2deg_',\n"," 'random_',\n"," 'ravel',\n"," 'real',\n"," 'reciprocal',\n"," 'reciprocal_',\n"," 'record_stream',\n"," 'refine_names',\n"," 'register_hook',\n"," 'reinforce',\n"," 'relu',\n"," 'relu_',\n"," 'remainder',\n"," 'remainder_',\n"," 'rename',\n"," 'rename_',\n"," 'renorm',\n"," 'renorm_',\n"," 'repeat',\n"," 'repeat_interleave',\n"," 'requires_grad',\n"," 'requires_grad_',\n"," 'reshape',\n"," 'reshape_as',\n"," 'resize',\n"," 'resize_',\n"," 'resize_as',\n"," 'resize_as_',\n"," 'resolve_conj',\n"," 'resolve_neg',\n"," 'retain_grad',\n"," 'retains_grad',\n"," 'roll',\n"," 'rot90',\n"," 'round',\n"," 'round_',\n"," 'rsqrt',\n"," 'rsqrt_',\n"," 'scatter',\n"," 'scatter_',\n"," 'scatter_add',\n"," 'scatter_add_',\n"," 'select',\n"," 'set_',\n"," 'sgn',\n"," 'sgn_',\n"," 'shape',\n"," 'share_memory_',\n"," 'short',\n"," 'sigmoid',\n"," 'sigmoid_',\n"," 'sign',\n"," 'sign_',\n"," 'signbit',\n"," 'sin',\n"," 'sin_',\n"," 'sinc',\n"," 'sinc_',\n"," 'sinh',\n"," 'sinh_',\n"," 'size',\n"," 'slogdet',\n"," 'smm',\n"," 'softmax',\n"," 'solve',\n"," 'sort',\n"," 'sparse_dim',\n"," 'sparse_mask',\n"," 'sparse_resize_',\n"," 'sparse_resize_and_clear_',\n"," 'split',\n"," 'split_with_sizes',\n"," 'sqrt',\n"," 'sqrt_',\n"," 'square',\n"," 'square_',\n"," 'squeeze',\n"," 'squeeze_',\n"," 'sspaddmm',\n"," 'std',\n"," 'stft',\n"," 'storage',\n"," 'storage_offset',\n"," 'storage_type',\n"," 'stride',\n"," 'sub',\n"," 'sub_',\n"," 'subtract',\n"," 'subtract_',\n"," 'sum',\n"," 'sum_to_size',\n"," 'svd',\n"," 'swapaxes',\n"," 'swapaxes_',\n"," 'swapdims',\n"," 'swapdims_',\n"," 'symeig',\n"," 't',\n"," 't_',\n"," 'take',\n"," 'take_along_dim',\n"," 'tan',\n"," 'tan_',\n"," 'tanh',\n"," 'tanh_',\n"," 'tensor_split',\n"," 'tile',\n"," 'to',\n"," 'to_dense',\n"," 'to_mkldnn',\n"," 'to_sparse',\n"," 'to_sparse_csr',\n"," 'tolist',\n"," 'topk',\n"," 'trace',\n"," 'transpose',\n"," 'transpose_',\n"," 'triangular_solve',\n"," 'tril',\n"," 'tril_',\n"," 'triu',\n"," 'triu_',\n"," 'true_divide',\n"," 'true_divide_',\n"," 'trunc',\n"," 'trunc_',\n"," 'type',\n"," 'type_as',\n"," 'unbind',\n"," 'unflatten',\n"," 'unfold',\n"," 'uniform_',\n"," 'unique',\n"," 'unique_consecutive',\n"," 'unsafe_chunk',\n"," 'unsafe_split',\n"," 'unsafe_split_with_sizes',\n"," 'unsqueeze',\n"," 'unsqueeze_',\n"," 'values',\n"," 'var',\n"," 'vdot',\n"," 'view',\n"," 'view_as',\n"," 'vsplit',\n"," 'where',\n"," 'xlogy',\n"," 'xlogy_',\n"," 'xpu',\n"," 'zero_']"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"kxgQRo18oD0E"},"source":["返回很多，我们直接排除掉一些Python中特殊方法（以__开头和结束的）和私有方法（以_开头的，直接看几个比较主要的属性：\n","\n","`.is_leaf`：记录是否是`叶子节点`。通过这个属性来确定这个变量的类型\n","在官方文档中所说的“graph leaves”，“leaf variables”，都是指像`x`，`y`这样的手动创建的、而非运算得到的变量，这些变量成为`创建变量`。\n","\n","像`z`这样的，是通过计算后得到的结果称为`结果变量`。\n","\n","一个变量是创建变量还是结果变量是通过`.is_leaf`来获取的。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jo1Qf_2KoVCL","executionInfo":{"status":"ok","timestamp":1637064206083,"user_tz":-480,"elapsed":901,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"a4627cac-8f07-4396-c109-2185c618a09e"},"source":["print(\"x.is_leaf=\"+str(x.is_leaf))\n","print(\"z.is_leaf=\"+str(z.is_leaf))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x.is_leaf=True\n","z.is_leaf=False\n"]}]},{"cell_type":"markdown","metadata":{"id":"gKHNcphLodKM"},"source":["`x`是`手动创建`的没有通过计算，所以他被认为是一个`叶子节点`也就是一个创建变量，而`z`是通过`x`与`y`的一系列`计算得到`的，所以不是叶子结点也就是`结果变量`。\n","\n","为什么我们执行`z.backward()`方法会更新`x.grad`和`y.grad`呢？\n","`.grad_fn`属性记录的就是这部分的操作，虽然`.backward()`方法也是CPP实现的，但是可以通过Python来进行简单的探索。\n","\n","`grad_fn`：记录并且编码了完整的计算历史"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtJ2m1C1oo4H","executionInfo":{"status":"ok","timestamp":1637064288227,"user_tz":-480,"elapsed":976,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"072b440c-ea65-47ff-9b10-ee8e208b10b1"},"source":["z.grad_fn"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AddBackward0 at 0x7f3b050fa910>"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"Vvr_Ho_loqKw"},"source":["`grad_fn`是一个`AddBackward0`类型的变量 `AddBackward0`这个类也是用Cpp来写的，但是我们从名字里就能够大概知道，他是加法(ADD)的反向传播（Backward），看看里面有些什么东西"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xPtfpNfDou7Y","executionInfo":{"status":"ok","timestamp":1637064324444,"user_tz":-480,"elapsed":799,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"fd8e7a16-41e8-4428-abf0-bc153f6c5a18"},"source":["dir(z.grad_fn)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__call__',\n"," '__class__',\n"," '__delattr__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '_register_hook_dict',\n"," '_saved_alpha',\n"," 'metadata',\n"," 'name',\n"," 'next_functions',\n"," 'register_hook',\n"," 'requires_grad']"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"zBmn3qXAowNH"},"source":["`next_functions`就是`grad_fn`的精华"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugL9-KBXo2jf","executionInfo":{"status":"ok","timestamp":1637113846180,"user_tz":-480,"elapsed":1370,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"a9157eb2-ed2e-4642-8329-3b12b6335514"},"source":["print(z.grad_fn)\n","print(z.grad_fn.next_functions)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["<AddBackward0 object at 0x7fccd0e5df50>\n","((<PowBackward0 object at 0x7fccd0e5ded0>, 0), (<PowBackward0 object at 0x7fccd0e5d450>, 0))\n"]}]},{"cell_type":"markdown","metadata":{"id":"HqPQmps0o1tH"},"source":["`next_functions`是一个tuple of tuple of PowBackward0 and int。\n","\n","为什么是2个tuple ？\n","因为我们的操作是`z= x**2+y**3` 刚才的`AddBackward0`是相加，而前面的操作是乘方 `PowBackward0`。tuple第一个元素就是x相关的操作记录。\n","\n","即，第一个PowBackward0是x的操作记录，第二个是y的操作记录，`z=x**2+y**3`，所以`z.grad_fn.next_functions`就是记录了`z.grad_fn`的上一个操作记录（或者对反向传播来说是下一个），基于这种记录函数的办法，才能完成自动求导。"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bmI0T3v0jttp","executionInfo":{"status":"ok","timestamp":1637113884626,"user_tz":-480,"elapsed":530,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"17d4e1cf-03c4-43dd-824e-0ab0172bf3ad"},"source":["x1=torch.ones_like(x)\n","z=x**2+y**3+x1\n","z.backward(torch.ones_like(x))\n","print(z.grad_fn)\n","print(x1.grad_fn)\n","print(z.grad_fn.next_functions)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["<AddBackward0 object at 0x7fccd0df6910>\n","None\n","((<AddBackward0 object at 0x7fccd0e10490>, 0), (None, 0))\n"]}]},{"cell_type":"markdown","metadata":{"id":"BkRf1G0AkA9C"},"source":["可以看到，新加了一个`x1`，由于x1这个tensor是叶子节点，对于`z.grad_fn.next_functions`，就包括了`x1.grad_fn`，为`none`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0_4vmBVo79y","executionInfo":{"status":"ok","timestamp":1637113995071,"user_tz":-480,"elapsed":546,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"6388fb7c-4ef0-481b-e332-62bf8b2e3f83"},"source":["xg = z.grad_fn.next_functions[0][0] ## z.grad_fn.next_functions[0][0]（z.grad_fn.next_functions的第1个元组的第一个元素，那就是x.grad_fn属性）\n","dir(xg)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__call__',\n"," '__class__',\n"," '__delattr__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__ne__',\n"," '__new__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," '_register_hook_dict',\n"," '_saved_alpha',\n"," 'metadata',\n"," 'name',\n"," 'next_functions',\n"," 'register_hook',\n"," 'requires_grad']"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"NuxYBaA7o-xx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637114165208,"user_tz":-480,"elapsed":532,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"65120405-8864-4115-b81a-d89674f952cb"},"source":["x_leaf=xg.next_functions[0][0]\n","x_leaf=x_leaf.next_functions[0][0] ## 这里由于我中间加了一个实验，所以多了一层，所以要对z找两个 .next_function[0][0]才回到x.grad_fn\n","print(type(x_leaf))\n","print(x.is_leaf)"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'AccumulateGrad'>\n","True\n"]}]},{"cell_type":"markdown","metadata":{"id":"YqJJUFv1pDJK"},"source":["在PyTorch的反向图计算中，`AccumulateGrad`类型代表的就是叶子节点类型，也就是计算图终止节点。`AccumulateGrad`类中有一个`.variable`属性指向叶子节点。\n","\n","这个`AccumulatedGrad`类可以理解成是在`叶子节点`的tensor的`.next_function`属性，相当于一个链表的终止指针，指向了一个none"]},{"cell_type":"code","metadata":{"id":"VobKGmkLpFGq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637114370515,"user_tz":-480,"elapsed":572,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"1b3c5962-9ed1-455b-ce83-095106853b43"},"source":["x_leaf.variable "],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8350, 0.8258, 0.7220, 0.4577, 0.1289],\n","        [0.2633, 0.6853, 0.7088, 0.1296, 0.1095],\n","        [0.1328, 0.4573, 0.6789, 0.1820, 0.8575],\n","        [0.5902, 0.6608, 0.2611, 0.2565, 0.0670],\n","        [0.1765, 0.6248, 0.1788, 0.9213, 0.0177]], requires_grad=True)"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"Gp9M-qP_eqZB"},"source":["这个`.variable`的属性就是我们的生成的变量`x`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"47R4yIGnexrQ","executionInfo":{"status":"ok","timestamp":1637114528214,"user_tz":-480,"elapsed":520,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"a4078bf5-abfc-4b96-ad20-ceed58d1af18"},"source":["print(\"x_leaf.variable的id:\"+str(id(x_leaf.variable)))\n","print(\"x的id:\"+str(id(x)))"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["x_leaf.variable的id:140517655156400\n","x的id:140517655156400\n"]}]},{"cell_type":"markdown","metadata":{"id":"Xq19NHgWewGw"},"source":["assert(id(x_leaf.variable)==id(x))"]},{"cell_type":"markdown","metadata":{"id":"f7SKrbWKe0Lo"},"source":["这样整个规程就很清晰了：\n","\n","1. 当我们执行`z.backward()`的时候。这个操作将调用z里面的`grad_fn`这个属性，执行求导的操作。\n","2. 这个操作将`遍历grad_fn`的`next_functions`，然后分别取出里面的Function（AccumulateGrad），执行求导操作。这部分是一个`递归`的过程直到最后类型为`叶子节点`。\n","3. 计算出结果以后，将结果保存到他们对应的variable这个变量所引用的对象（x和y）的 `grad`这个属性里面。\n","4. 求导结束。所有的叶节点的grad变量都得到了相应的更新\n","\n","最终当我们执行完`z.backward()`之后，a和b里面的grad值就得到了更新。\n","\n","简单一句话：遍历`z.next_functions`直到叶子节点，保存所有中间计算到的梯度到各自的`.grad`里面"]},{"cell_type":"markdown","metadata":{"id":"6g5JZSyie7AC"},"source":["## 扩展Autograd\n","如果需要`自定义autograd`扩展新的功能，就需要`扩展Function类`。因为Function使用autograd来计算结果和梯度，并对操作历史进行编码。\n","在Function类中最主要的方法就是`forward()`和`backward()`他们分别代表了`前向传播`和`反向传播`。\n","\n","一个自定义的Function需要一下三个方法：\n","\n","    __init__ (optional)：如果这个操作需要额外的参数则需要定义这个Function的构造函数，不需要的话可以忽略。\n","    \n","    forward()：执行前向传播的计算代码\n","    \n","    backward()：反向传播时梯度计算的代码。 参数的个数和forward返回值的个数一样，每个参数代表传回到此操作的梯度。\n","        \n","\n"]},{"cell_type":"code","metadata":{"id":"VaOYS7bipCq6","executionInfo":{"status":"ok","timestamp":1637114820452,"user_tz":-480,"elapsed":564,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}}},"source":["# 引入Function便于扩展\n","from torch.autograd.function import Function"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzpfPyS0pfoB","executionInfo":{"status":"ok","timestamp":1637114945483,"user_tz":-480,"elapsed":683,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}}},"source":["# 定义一个乘以常数的操作(输入参数是张量)\n","# 方法必须是静态方法，所以要加上@staticmethod \n","class MulConstant(Function):\n","    @staticmethod ## 声明是静态方法\n","    def forward(ctx, tensor, constant):\n","        # ctx 用来保存信息这里类似self，并且ctx的属性可以在backward中调用\n","        ctx.constant=constant\n","        return tensor *constant\n","    @staticmethod \n","    def backward(ctx, grad_output):\n","        # 返回的参数要与输入的参数一样.\n","        # 第一个输入为3x3的张量，第二个为一个常数\n","        # 常数的梯度必须是 None.\n","        return grad_output, None "],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuwbzgj3pzhZ"},"source":["定义完我们的新操作后，我们来进行测试"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S6y1GkCbp0wg","executionInfo":{"status":"ok","timestamp":1637114947371,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"650701c3-3a7f-4976-cb96-9deea1cceec8"},"source":["a=torch.rand(3,3,requires_grad=True)\n","b=MulConstant.apply(a,5)\n","print(\"a:\"+str(a))\n","print(\"b:\"+str(b)) # b为a的元素乘以5"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["a:tensor([[0.7246, 0.6352, 0.3572],\n","        [0.4215, 0.1447, 0.5872],\n","        [0.7652, 0.3028, 0.3580]], requires_grad=True)\n","b:tensor([[3.6232, 3.1758, 1.7859],\n","        [2.1073, 0.7236, 2.9358],\n","        [3.8259, 1.5140, 1.7899]], grad_fn=<MulConstantBackward>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"J0DYO7Zkp-ZA"},"source":["反向传播，返回值不是标量，所以`backward`方法需要参数"]},{"cell_type":"code","metadata":{"id":"T3iCrxlIp_8o","executionInfo":{"status":"ok","timestamp":1637114975119,"user_tz":-480,"elapsed":552,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}}},"source":["b.backward(torch.ones_like(a))"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcDWLWKmqDcw","executionInfo":{"status":"ok","timestamp":1637114991822,"user_tz":-480,"elapsed":532,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"b1acbab3-7e1c-4549-a778-3099a1805df3"},"source":["a.grad"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1.],\n","        [1., 1., 1.],\n","        [1., 1., 1.]])"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"ExjY7zQ4qCiw"},"source":["梯度为1。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"C2IPeDRzqLSQ"},"source":["## 总结\n","①autograd是基于tensor的`.next_function`属性来记录操作，`.grad来保存结果`\n","\n","②对于计算结果如果不是一个标量，那么反向传播的时候需要指定一个和要求的梯度的叶子节点同型的tensor\n","\n","③扩展Autog"]}]}