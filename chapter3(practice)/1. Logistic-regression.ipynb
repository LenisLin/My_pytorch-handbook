{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Logistic-regression.ipynb","provenance":[],"mount_file_id":"1n_IY1CuPv2_jHf6bJHCp3aSTbYqbu-Op","authorship_tag":"ABX9TyOOS612CSY0uvAiGmrLsLIa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"3J-tL9H6deX6","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1638671133859,"user_tz":-480,"elapsed":570,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"2431c644-37af-4ef5-f286-7b44e5753806"},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import os\n","import pandas as pd\n","torch.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.10.0+cu111'"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"_hAN6btbSRTX"},"source":["# logistic回归实战\n","在这一章里面，我们将处理一下结构化数据，并使用logistic回归对结构化数据进行简单的分类。\n","##  logistic回归(logistic regression)介绍\n","`logistic回归`是一种广义线性回归（generalized linear model），与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有` wx + b，其中w和b是待求参数`，其区别在于他们的因变量不同，`多重线性回归`直接`将wx+b作为因变量`，即`y =wx+b`,而`logistic回归`则通过函数L将wx+b对应一个隐状态p，`p =L(wx+b`然后`根据p 与1-p的大小决定因变量的值`。如果`L是logistic函数，就是logistic回归`，如果L是多项式函数就是多项式回归。\n","\n","即：logistic regression在计算了wx+b后，还要经过一层函数（吴恩达机器学习中为sigmoid函数，实际上也可以是softmax函数），将连续值转化为标签概率值\n","\n","说的更通俗一点，就是logistic回归会在`线性回归后再加一层logistic函数`的调用。\n","\n","logistic回归主要是进行二分类预测，我们在激活函数时候讲到过 Sigmod函数，`Sigmod函数是最常见的logistic函数`，因为Sigmod函数的输出的是是对于0~1之间的概率值，当概率大于0.5预测为1，小于0.5预测为0。\n","\n","下面我们就来使用公开的数据来进行介绍"]},{"cell_type":"markdown","metadata":{"id":"beqT5oOESxm8"},"source":["## UCI German Credit  数据集\n","\n","UCI German Credit是UCI的德国信用数据集，里面有原数据和数值化后的数据。\n","\n","German Credit数据是根据个人的银行贷款信息和申请客户贷款逾期发生情况来预测贷款违约倾向的数据集，数据集包含24个维度的，1000条数据，\n","\n","在这里我们直接使用处理好的数值化的数据，作为展示。\n","\n","[地址](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)"]},{"cell_type":"markdown","metadata":{"id":"v567AgfHTDOb"},"source":["## 代码实战\n","我们这里使用的 german.data-numeric是numpy处理好数值化数据，我们直接使用numpy的load方法读取即可"]},{"cell_type":"code","metadata":{"id":"hKDR-5tSTFyZ"},"source":["data=np.loadtxt(\"/content/drive/MyDrive/pytorch-handbook/chapter3(practice )/german.data-numeric\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aOD_BZodYJF5"},"source":["数据读取完成后我们要对数据做一下归一化的处理"]},{"cell_type":"code","metadata":{"id":"CxbHlSTqYKoi"},"source":["n,l=data.shape ## n为行数，l为列数\n","## 对每一列用z-score来归一化\n","for j in range(l-1):\n","    meanVal=np.mean(data[:,j])\n","    stdVal=np.std(data[:,j])\n","    data[:,j]=(data[:,j]-meanVal)/stdVal"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ovov9OIrZ_IV"},"source":["打乱数据"]},{"cell_type":"code","metadata":{"id":"kmIWLv0PaBrs"},"source":["np.random.shuffle(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dIX92-35aApE"},"source":["区分训练集和测试集，由于这里没有验证集，所以我们直接使用测试集的准确度作为评判好坏的标准。\n","\n","区分规则：900条用于训练，100条作为测试\n","\n","german.data-numeric的格式为，前24列为24个维度，最后一个为要打的标签（0，1）（y），所以我们将数据和标签一起区分出来"]},{"cell_type":"code","metadata":{"id":"yPY8aMC1aJfW"},"source":["## 定义training set 和 test set\n","train_data=data[:900,:l-1] ## 1-900行\n","test_data=data[900:,:l-1] ## 901-1000\n","\n","train_lab=data[:900,l-1]-1 ## 对应train_data的最后一列\n","test_lab=data[900:,l-1]-1 ## 减一是因为数据集的label是1-2，但sigmoid接受的范围是0-1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNmVZEWLbBW4"},"source":["## 下面我们定义模型\n","class LogisticRegression(nn.Module):\n","  def __init__(self):\n","    ## super().__init__()\n","    super(LogisticRegression,self).__init__() ## 继承nn.Module中的__init__方法，在调用子类LogisticRegression的__init__的同时，还会调用父类nn.Module中的__init__的内容\n","    self.fc=nn.Linear(24,2) ## 这里的24和dataset的features一致,2对应二分类的两个概率\n","\n","  def forward(self,x): ## x 为 input\n","    out=self.fc(x) ## 经过一层线性层，得到多元回归的结果\n","    out=torch.sigmoid(out) ## 经过sigmoid激活，将多元回归的结果转化为标签\n","    return out\n","\n","def test(pred,lab):\n","    t=pred.max(-1)[1]==lab ## 每个实例预测最大的预测概率\n","    return torch.mean(t.float())\n","\n","## 其实这里比较奇怪，按照吴恩达在机器学习中的降解，logistic regression的h(x)是预测y=1时的概率，这里定义网络实际上定义成nn.Linear(24,1)就可以了"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63Sqm8rLbD4G","executionInfo":{"status":"ok","timestamp":1638672741723,"user_tz":-480,"elapsed":581,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"19b03287-558a-43e1-86e0-db4db3be349f"},"source":["## super() 介绍\n","class A():\n","    def fortest(self):\n","        print('Call class A')\n","        print('Leave class A')\n","\n","class B1(A):\n","    def fortest(self):\n","        print('Call class B1')\n","        print('Leave class B1')\n","\n","\n","class B2(A): ## 有继承\n","    def fortest(self):\n","        print('Call class B2')\n","        super(B2,self).fortest()\n","        print('Leave class B2')\n","sample1=B1()\n","sample2=B2()\n","\n","print(sample1.fortest()) ## 没有继承，所以调用sample1对应的子类B1，只调用自己的fortest方法（父类的fortest方法被覆盖）\n","print('\\n')\n","print(sample2.fortest()) ## 继承了，调用子类B2的fortest方法的同时还调用了父类的fortest方法，不会覆盖父类"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Call class B1\n","Leave class B1\n","None\n","\n","\n","Call class B2\n","Call class A\n","Leave class A\n","Leave class B2\n","None\n"]}]},{"cell_type":"code","metadata":{"id":"d9FTulgGkg6K"},"source":["## 初始化网络\n","net=LogisticRegression() \n","criterion=nn.CrossEntropyLoss() # 使用CrossEntropyLoss损失\n","optm=torch.optim.Adam(net.parameters()) # Adam优化\n","epochs=1000 # 训练1000次"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCawNyr0krBA","executionInfo":{"status":"ok","timestamp":1638673929354,"user_tz":-480,"elapsed":1129,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"426dd964-60b0-465b-d575-3dea76d72c15"},"source":["## training\n","for i in range(epochs):\n","    # 指定模型为训练模式，计算梯度\n","    net.train()\n","    # 输入值都需要转化成torch的Tensor\n","    x=torch.from_numpy(train_data).float()\n","    y=torch.from_numpy(train_lab).long()\n","    y_hat=net(x)\n","    loss=criterion(y_hat,y) # 计算损失\n","    optm.zero_grad() # 前一步的损失清零\n","    loss.backward() # 反向传播\n","    optm.step() # 优化\n","    if (i+1)%100 ==0 : # 这里我们每100次输出相关的信息\n","        # 指定模型为计算模式\n","        net.eval()\n","        test_in=torch.from_numpy(test_data).float()\n","        test_l=torch.from_numpy(test_lab).long()\n","        test_out=net(test_in)\n","        # 使用我们的测试函数计算准确率\n","        accu=test(test_out,test_l)\n","        print(\"Epoch:{},Loss:{:.4f},Accuracy：{:.2f}\".format(i+1,loss.item(),accu))\n","    ## 训练完成"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:100,Loss:0.6475,Accuracy：0.71\n","Epoch:200,Loss:0.6187,Accuracy：0.76\n","Epoch:300,Loss:0.6006,Accuracy：0.77\n","Epoch:400,Loss:0.5882,Accuracy：0.77\n","Epoch:500,Loss:0.5789,Accuracy：0.77\n","Epoch:600,Loss:0.5717,Accuracy：0.76\n","Epoch:700,Loss:0.5658,Accuracy：0.77\n","Epoch:800,Loss:0.5609,Accuracy：0.78\n","Epoch:900,Loss:0.5567,Accuracy：0.77\n","Epoch:1000,Loss:0.5531,Accuracy：0.77\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Lb_1ZXalCJ5","executionInfo":{"status":"ok","timestamp":1638674232983,"user_tz":-480,"elapsed":509,"user":{"displayName":"Yuxiang Lin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10241123578442902660"}},"outputId":"e2568d6b-4547-4702-ea15-f2e1dabb52f1"},"source":["## some details\n","a=test_out[0:5,:]\n","print(a,'\\n')\n","\n","## .max()\n","print(a.max(0),'\\n') ## .max(0)，对每列求最值\n","print(a.max(1),'\\n') ## .max(1)，对每行求最值\n","print(a.max(1)[0],'\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.8997, 0.2912],\n","        [0.3434, 0.7658],\n","        [0.9145, 0.0871],\n","        [0.5292, 0.4603],\n","        [0.9836, 0.0072]], grad_fn=<SliceBackward0>) \n","\n","torch.return_types.max(\n","values=tensor([0.9836, 0.7658], grad_fn=<MaxBackward0>),\n","indices=tensor([4, 1])) \n","\n","torch.return_types.max(\n","values=tensor([0.8997, 0.7658, 0.9145, 0.5292, 0.9836], grad_fn=<MaxBackward0>),\n","indices=tensor([0, 1, 0, 0, 0])) \n","\n","tensor([0.8997, 0.7658, 0.9145, 0.5292, 0.9836], grad_fn=<MaxBackward0>) \n","\n"]}]}]}